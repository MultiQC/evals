{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Parquet on s3 performance and partitioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"s3://megaqc-test/datasets\"\n",
    "\n",
    "# Configuration parameters\n",
    "NUM_RUNS = 10  # Can be scaled up to millions in real case\n",
    "NUM_MODULES = 10  # Fixed across runs\n",
    "NUM_SAMPLES_PER_MODULE = 100  # Can be 10 to 1000\n",
    "NUM_METRICS_PER_MODULE = 20  # Can be 10 to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    \"\"\"Generate a random string of fixed length\"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "\n",
    "def generate_metric_metadata():\n",
    "    \"\"\"Generate metadata for a metric\"\"\"\n",
    "    return {\n",
    "        \"min\": random.uniform(0, 10),\n",
    "        \"max\": random.uniform(90, 100),\n",
    "        \"scale\": random.choice([\"linear\", \"log\"]),\n",
    "        \"color\": f\"#{random.randint(0, 0xFFFFFF):06x}\",\n",
    "        \"type\": random.choice([\"numeric\", \"categorical\", \"percentage\"]),\n",
    "        \"namespace\": random.choice([\"performance\", \"quality\", \"resource\"]),\n",
    "        \"placement\": random.choice([\"primary\", \"secondary\", \"tertiary\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_value_metadata(value):\n",
    "    \"\"\"Generate metadata for a value\"\"\"\n",
    "    return {\n",
    "        \"unmodified_value\": value,\n",
    "        \"formatted_value\": f\"{value:.2f}\" if isinstance(value, float) else str(value),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_sample_data(num_metrics):\n",
    "    \"\"\"Generate data for a single sample\"\"\"\n",
    "    sample_id = generate_random_string()\n",
    "    metrics = {}\n",
    "\n",
    "    for i in range(num_metrics):\n",
    "        metric_name = f\"metric_{i}\"\n",
    "        value = random.uniform(0, 100)\n",
    "        metrics[metric_name] = {\n",
    "            \"value\": value,\n",
    "            \"metadata\": generate_value_metadata(value),\n",
    "        }\n",
    "\n",
    "    return {\"sample_id\": sample_id, \"metrics\": metrics}\n",
    "\n",
    "\n",
    "def generate_module_data(module_index, num_samples, num_metrics):\n",
    "    \"\"\"Generate data for a single module\"\"\"\n",
    "    samples = [generate_sample_data(num_metrics) for _ in range(num_samples)]\n",
    "\n",
    "    metrics_metadata = {}\n",
    "    for i in range(num_metrics):\n",
    "        metric_name = f\"metric_{i}\"\n",
    "        metrics_metadata[metric_name] = generate_metric_metadata()\n",
    "\n",
    "    return {\n",
    "        \"module_id\": f\"module_{module_index}\",\n",
    "        \"name\": f\"Module {module_index}\",\n",
    "        \"url\": f\"http://example.com/module/{module_index}\",\n",
    "        \"comment\": f\"This is module {module_index}\",\n",
    "        \"metrics_metadata\": metrics_metadata,\n",
    "        \"samples\": samples,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_run_data(\n",
    "    run_index, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "):\n",
    "    \"\"\"Generate data for a single run\"\"\"\n",
    "    modules = [\n",
    "        generate_module_data(i, num_samples_per_module, num_metrics_per_module)\n",
    "        for i in range(num_modules)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"run_id\": f\"run_{run_index}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"modules\": modules,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_all_data(\n",
    "    num_runs, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "):\n",
    "    \"\"\"Generate all runs data\"\"\"\n",
    "    return [\n",
    "        generate_run_data(\n",
    "            i, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "        )\n",
    "        for i in range(num_runs)\n",
    "    ]\n",
    "\n",
    "\n",
    "def flatten_hierarchical_data(data):\n",
    "    \"\"\"Convert hierarchical data to flat format for Parquet\"\"\"\n",
    "    flat_records = []\n",
    "\n",
    "    for run in data:\n",
    "        run_id = run[\"run_id\"]\n",
    "        timestamp = run[\"timestamp\"]\n",
    "\n",
    "        for module in run[\"modules\"]:\n",
    "            module_id = module[\"module_id\"]\n",
    "            module_name = module[\"name\"]\n",
    "            module_url = module[\"url\"]\n",
    "            module_comment = module[\"comment\"]\n",
    "\n",
    "            for sample in module[\"samples\"]:\n",
    "                sample_id = sample[\"sample_id\"]\n",
    "\n",
    "                for metric_name, metric_data in sample[\"metrics\"].items():\n",
    "                    value = metric_data[\"value\"]\n",
    "                    unmodified_value = metric_data[\"metadata\"][\"unmodified_value\"]\n",
    "                    formatted_value = metric_data[\"metadata\"][\"formatted_value\"]\n",
    "\n",
    "                    # Get metric metadata\n",
    "                    metric_metadata = module[\"metrics_metadata\"].get(metric_name, {})\n",
    "\n",
    "                    flat_records.append(\n",
    "                        {\n",
    "                            \"run_id\": run_id,\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"module_id\": module_id,\n",
    "                            \"module_name\": module_name,\n",
    "                            \"module_url\": module_url,\n",
    "                            \"module_comment\": module_comment,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"metric_name\": metric_name,\n",
    "                            \"value\": value,\n",
    "                            \"unmodified_value\": unmodified_value,\n",
    "                            \"formatted_value\": formatted_value,\n",
    "                            \"metric_min\": metric_metadata.get(\"min\"),\n",
    "                            \"metric_max\": metric_metadata.get(\"max\"),\n",
    "                            \"metric_scale\": metric_metadata.get(\"scale\"),\n",
    "                            \"metric_color\": metric_metadata.get(\"color\"),\n",
    "                            \"metric_type\": metric_metadata.get(\"type\"),\n",
    "                            \"metric_namespace\": metric_metadata.get(\"namespace\"),\n",
    "                            \"metric_placement\": metric_metadata.get(\"placement\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    return flat_records\n",
    "\n",
    "\n",
    "def store_in_parquet(data, parquet_dir):\n",
    "    \"\"\"Store the flattened data in Parquet format\"\"\"\n",
    "    print(\"Flattining data...\")\n",
    "    flat_data = flatten_hierarchical_data(data)\n",
    "    print(\"Creating DataFrame...\")\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    print(\"Creating Parquet Table...\")\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    # Write to Parquet file with partitioning\n",
    "    start_time = time.time()\n",
    "    print(f\"Writing to Parquet file {parquet_dir}...\")\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=parquet_dir,\n",
    "        partition_cols=[\"run_id\"],\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_parquet_duckdb(parquet_dir):\n",
    "    \"\"\"Query Parquet files to retrieve specific metric values using DuckDB\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    # Use DuckDB to query the Parquet files directly\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM '{parquet_dir}/**/*.parquet'\n",
    "        WHERE metric_name = '{metric_name}'\n",
    "    \"\"\"\n",
    "    filtered_df = con.execute(query).fetchdf()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return filtered_df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_parquet_pyarrow(parquet_dir):\n",
    "    start_time = time.time()\n",
    "\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    # Read the Parquet files with partitioning information\n",
    "    dataset = ds.dataset(parquet_dir, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "    # Define filter condition for the metric name\n",
    "    filter_expr = (ds.field(\"metric_name\") == metric_name)\n",
    "    # Read the filtered data\n",
    "    table = dataset.to_table(filter=filter_expr)\n",
    "    # Convert to pandas DataFrame if needed\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_item_parquet_duckdb(parquet_dir):\n",
    "    \"\"\"Query Parquet files to retrieve specific items using DuckDB\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Filter by run_id and module_id\n",
    "    run_id = \"run_0\"\n",
    "    module_id = \"module_0\"\n",
    "\n",
    "    # Use DuckDB to query the Parquet files directly\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM '{parquet_dir}/**/*.parquet'\n",
    "        WHERE run_id = '{run_id}' AND module_id = '{module_id}'\n",
    "    \"\"\"\n",
    "    filtered_df = con.execute(query).fetchdf()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return filtered_df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_item_parquet_pyarrow(parquet_dir):\n",
    "    \"\"\"Query Parquet files to retrieve specific items using pyarrow\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Filter by run_id and module_id\n",
    "    run_id = \"run_0\"\n",
    "    module_id = \"module_0\"\n",
    "\n",
    "    # Read the Parquet files with partitioning information\n",
    "    dataset = ds.dataset(parquet_dir, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "    # Define filter condition for the metric name\n",
    "    filter_expr = (ds.field(\"run_id\") == run_id) & (ds.field(\"module_id\") == module_id)\n",
    "    # Read the filtered data\n",
    "    table = dataset.to_table(filter=filter_expr)\n",
    "    # Convert to pandas DataFrame if needed\n",
    "    df = table.to_pandas()\n",
    "    end_time = time.time()\n",
    "\n",
    "    return df, end_time - start_time    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample data...\n",
      "Generated 10 runs with 10 modules each\n",
      "Each module has 100 samples with 20 metrics\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating sample data...\")\n",
    "data = generate_all_data(\n",
    "    NUM_RUNS, NUM_MODULES, NUM_SAMPLES_PER_MODULE, NUM_METRICS_PER_MODULE\n",
    ")\n",
    "print(f\"Generated {NUM_RUNS} runs with {NUM_MODULES} modules each\")\n",
    "print(\n",
    "    f\"Each module has {NUM_SAMPLES_PER_MODULE} samples with {NUM_METRICS_PER_MODULE} metrics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Storing objects to the bucket s3://megaqc-test/datasets/2025-04-23-10-59-58 ---\n",
      "Flattining data...\n",
      "Creating DataFrame...\n",
      "Creating Parquet Table...\n",
      "Writing to Parquet file s3://megaqc-test/datasets/2025-04-23-10-59-58...\n",
      "Parquet storage time: 12.7854 seconds\n"
     ]
    }
   ],
   "source": [
    "# bucket_path = BUCKET_NAME + \"/\" + \"2025-04-23-10-18-02\"\n",
    "bucket_path = BUCKET_NAME + \"/\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"\\n--- Storing objects to the bucket {bucket_path} ---\")\n",
    "parquet_store_time = store_in_parquet(data, bucket_path)\n",
    "print(f\"Parquet storage time: {parquet_store_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet query single metric time (with pyarrow): 61.1109 seconds\n",
      "Parquet results count: 10000\n",
      "Parquet query single module time: 6.0639 seconds\n",
      "Parquet results count: 2000\n"
     ]
    }
   ],
   "source": [
    "parquet_results, parquet_query_time = query_single_metric_parquet_pyarrow(bucket_path)\n",
    "print(f\"Parquet query single metric time (with pyarrow): {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "parquet_results, parquet_query_time = query_single_item_parquet_pyarrow(bucket_path)\n",
    "print(f\"Parquet query single module time: {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
